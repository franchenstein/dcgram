{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGraM Algorithm\n",
    "\n",
    "This notebook implements the D-Markov with Clustering and Graph Minimization (DCGraM) Algorithm. Its objective is to model a discrete dynamical system using a Probabilistic Finite State Machine (PFSA). \n",
    "\n",
    "Given a sequence *X* over the alphabet $\\Sigma$ of length *N* that is an output of the original dynamical system, DCGraM works by:\n",
    "\n",
    "1. Creating a D-Markov model for the original system for a given *D*;\n",
    "2. Using a clustering algorithm on the D-Markov model states in order to create an initial partition;\n",
    "3. Using a graph minimization algorithm to refine the initial partition until the final reduced PFSA is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "First, it is necessary to create the directories that store the working files for the current system. The first cell sets the system's name and the tag to be used in the current run. The following cell only has to be ran when creating modeling a new system.  A directory is then created with this tag and inside it subdirectories that contain the sequence, PFSA and result files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import sequenceanalyzer as sa\n",
    "#import dmarkov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'ternary_even_shift'\n",
    "tag = 'v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(name):\n",
    "    os.makedirs(name)\n",
    "    os.makedirs(name + '/sequences')\n",
    "    os.makedirs(name + '/pfsa')\n",
    "    os.makedirs(name + '/results')\n",
    "    os.makedirs(name + '/results/probabilities')\n",
    "    os.makedirs(name + '/results/probabilities/conditional')\n",
    "    os.makedirs(name + '/results/cond_entropies')\n",
    "    os.makedirs(name + '/results/kldivergences')\n",
    "    os.makedirs(name + '/results/autocorrelations')\n",
    "    os.makedirs(name + '/results/prob_distances')\n",
    "    os.makedirs(name + '/results/plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "The next cell initializes the parameters that are used throughout the code. They are listed as:\n",
    "\n",
    "  * `N`: The original sequence length *N*, which is also the length of the sequences that are going to be generated by the PFSA generated by DCGraM;\n",
    "  * `drange`: range of values of *D* for which D-Markov and DCGraM machines that will be generated;\n",
    "  * `a`: value up to which the autocorrelation is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 10000000\n",
    "drange = range(4,11)\n",
    "a = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Sequence Analysis\n",
    "\n",
    "Make sure that the original sequence of length `N` is stored in the correct directory and run the cell to load it to `X`. After this, run the cells corresponding to the computation of the subsequence probabilities and the conditional probabilites for the value `d_max`, which is the last value in `drange`. Additional results can also be computed in the respective cells (autocorrelation and conditional entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Open original sequence from yaml file\n",
    "with open(name + '/sequences/original_len_' + str(N) + '_' + tag + '.yaml', 'r') as f:\n",
    "    X = yaml.load(f)\n",
    "    \n",
    "#Value up to which results are computed\n",
    "d_max = drange[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating subsequence probabilities\n",
      "L = 10\n",
      "Calculating probabilities of subsequences of length: 1\n",
      "Calculating probabilities of subsequences of length: 2\n",
      "Calculating probabilities of subsequences of length: 3\n",
      "Calculating probabilities of subsequences of length: 4\n",
      "Calculating probabilities of subsequences of length: 5\n",
      "Calculating probabilities of subsequences of length: 6\n",
      "Calculating probabilities of subsequences of length: 7\n",
      "Calculating probabilities of subsequences of length: 8\n",
      "Calculating probabilities of subsequences of length: 9\n",
      "Calculating probabilities of subsequences of length: 10\n",
      "*****************\n",
      "Probabilities calculated!\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "#Compute subsequence probabilities of occurrence up to length d_max\n",
    "p, alphabet = sa.calc_probs(X, d_max)\n",
    "with open(name + '/results/probabilities/original_' + tag + '.yaml', 'w') as f:\n",
    "    yaml.dump(p, f)\n",
    "with open(name + '/alphabet.yaml', 'w') as f:\n",
    "    yaml.dump(alphabet, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If p has been previously computed, use this cell to load the values\n",
    "if not p:\n",
    "    with open(name + '/results/probabilities/original_' + tag + '.csv', r) as f:\n",
    "        p = yaml.load(f)\n",
    "    with open(name + '/alphabet.yaml', 'r') as f:\n",
    "        alphabet = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute conditional probabilities of subsequences occurring after given each symbol of the alphabet\n",
    "#One of the two previous cells needs to be executed first.\n",
    "if p:\n",
    "    p_cond = sa.calc_cond_probs(p, alphabet, d_max) \n",
    "    p_cond.to_csv(name + '/results/probabilities/conditional/original_' + tag + '.csv')\n",
    "else:\n",
    "    print(\"Run a cell that either computes or opens the probabilities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If p_cond has been previously computed, use this cell to load the values\n",
    "if not p_cond:\n",
    "    p_cond = pd.read_csv(name + '/results/probabilities/conditional/original_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute conditional entropy\n",
    "if p and p_cond:\n",
    "    h = sa.calc_cond_entropy(p, p_cond, d_max)\n",
    "    h.to_csv(name + '/results/cond_entropies/original_' + tag + '.csv')\n",
    "else:\n",
    "    print(\"Run the conditional probabilities cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If p_cond has been previously computed, use this cell to load the values\n",
    "if not h:\n",
    "    h = pd.read_csv(name + '/results/cond_entropies/original_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute autocorrelation\n",
    "aut = sa.calc_autocorr(X, a)\n",
    "aut.to_csv(name + '/results/autocorrelations/original_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If aut has been previously computed, use this cell to load the values\n",
    "if not aut:\n",
    "    aut = pd.read_csv(name + '/results/autocorrelations/original_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D-Markov Machines\n",
    "\n",
    "The next step of DCGraM consists of generating D-Markov Machines for each value of *D* in `drange` defined above. The values of `p_cond` for each of these values is then needed, so it is necessary to compute it above. A D-Markov Machine is a PFSA with $|\\Sigma|^D$ states, each one labeled with one of the subsquences of length $D$. Given a state $\\omega = \\sigma_1\\sigma_2\\ldots\\sigma_D$, for each $\\sigma \\in \\Sigma$, it transitions to the state $\\sigma_2\\sigma_3\\ldots\\sigma_D\\sigma$ with probability $\\Pr(\\sigma|\\omega)$. This is done for all states in the D-Markov machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmark_machines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If the D-Markov machines have not been previously created, generate them with this cell\n",
    "for D in list(map(str,drange)):\n",
    "    dmark_machines.append(dmarkov.create(p_cond, D))\n",
    "    dmark_machines[-1].to_csv(name + '/pfsa/dmarkov_D' + D + '_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On the other hand, if there already are D-Markov machines, load them with this cell\n",
    "if not dmark_machines:\n",
    "    for D in drange:\n",
    "        dmark_machines.append(pd.read_csv(name + '/pfsa/dmarkov_D' + D + '_' + tag + '.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D-Markov Machine Analysis\n",
    "\n",
    "First of all, sequences should be generated from the D-Markov Machines. The same parameters computed in the analysis of the original sequence should be computed for the D-Markov Machines' sequences. Besides those parameters, the Kullback-Leibler Divergence and Distribution Distance between these sequences and the original sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dmark_seqs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate sequences:\n",
    "count = 0\n",
    "for machine in dmark_machines:\n",
    "    seq = machine.generate_sequence(N)\n",
    "    with open(name + '/sequences/dmarkov_D' + str(drange[count]) + '_' + tag + '.yaml', 'w') as f:\n",
    "        yaml.dump(seq, f)\n",
    "    dmark_seqs.append(seq)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If the sequences have been previously generated, load them here:\n",
    "if not dmark_seqs:\n",
    "    for D in list(map(str,drange)):\n",
    "        with open(name + '/sequences/dmarkov_D' + D + '_' + tag + '.yaml', 'w') as f:\n",
    "            dmark_seqs.append(yaml.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute subsequence probabilities of occurrence of the D-Markov sequences\n",
    "count = 0\n",
    "p_dmark = []\n",
    "for seq in dmark_seqs:\n",
    "    p_dm, alphabet = sa.calc_probs(seq, d_max)\n",
    "    p_dm.to_csv(name + '/results/probabilities/dmarkov_D'+ str(drange[count])  + '_' + tag + '.csv')\n",
    "    p_dmark.append(p_dm)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If p_dmark has been previously computed, use this cell to load the values\n",
    "if not p_dmark:\n",
    "    for D in list(map(str,drange)):\n",
    "        p_dm = pd.read_csv(name + '/results/probabilities/dmarkov_D' + D + '_' + tag + '.csv')\n",
    "        p_dmark.append(p_dm)\n",
    "    with open(name + '/alphabet.yaml', 'r') as f:\n",
    "        alphabet = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute conditional probabilities of subsequences occurring after given each symbol of the alphabet\n",
    "#One of the two previous cells needs to be executed first.\n",
    "p_cond_dmark = []\n",
    "count = 0\n",
    "if p_dmark:\n",
    "    for p_dm in p_dmark:\n",
    "        p_cond_dm = sa.calc_cond_probs(p_dm, alphabet, d_max) \n",
    "        p_cond_dm.to_csv(name + '/results/probabilities/conditional/dmarkov_D' + str(drange[count]) + '_' + tag + '.csv')\n",
    "        p_cond_dmark.append(p_cond_dm)\n",
    "        count += 1\n",
    "else:\n",
    "    print(\"Run a cell that either computes or opens the probabilities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If p_cond has been previously computed, use this cell to load the values\n",
    "if not p_cond_dmark:\n",
    "    for D in list(map(str,drange)):\n",
    "        p_cond_dmark.append(pd.read_csv(name + '/results/probabilities/conditional/dmarkov_D' + D + '_' + tag + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute conditional entropy\n",
    "count = 0\n",
    "h_dmark = []\n",
    "if p_dmark and p_cond_dmark:\n",
    "    for p_dm in p_dmark:\n",
    "        h_dm = sa.calc_cond_entropy(p_dm, p_cond_dmark[count], d_max)\n",
    "        h_dm.to_csv(name + '/results/cond_entropies/dmarkov_D' + str(drange[count]) + '_' + tag + '.csv')\n",
    "        h_dmark.append(h_dm)\n",
    "        count += 1\n",
    "else:\n",
    "    print(\"Run the conditional probabilities cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If h_dmark has been previously computed, use this cell to load the values\n",
    "if not h_dmark:\n",
    "    for D in list(map(str,drange)):\n",
    "        h_dmark.append(pd.read_csv(name + '/results/cond_entropies/dmarkov_D' + D + '_' + tag + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute autocorrelation\n",
    "aut_dmark = []\n",
    "count = 0\n",
    "for dseq in dmark_seqs:\n",
    "    aut_dm = sa.calc_autocorr(dseq, a)\n",
    "    aut_dm.to_csv(name + '/results/autocorrelations/dmarkov_D' + str(drange[count]) + '_'  + tag + '.csv')\n",
    "    aut_dmark.append(aut_dm)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If aut has been previously computed, use this cell to load the values\n",
    "if not aut_dmark:\n",
    "    for D in list(map(str,drange)):\n",
    "        aut_dmark.append(pd.read_csv(name + '/results/autocorrelations/dmarkov_D' + D + '_' + tag + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute the Kullback-Leibler Divergence between the sequences generated by the D-Markov Machines and the original\n",
    "#sequence.\n",
    "kld_dmark = []\n",
    "for dseq in dmark_seqs:\n",
    "    kld_dm = sa.calc_kld(dseq, X, d_max)\n",
    "    kld_dmark.append(kld_dm)\n",
    "    \n",
    "kld_dmark.to_csv(name + '/results/kldivergences/dmarkov_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If the D-Markov Kullback-Leibler divergence has been previously computed, use this cell to load the values\n",
    "if not kld_dmark:\n",
    "    kld_dmark = pd.read_csv(name + '/results/kldivergences/dmarkov_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute the Probability Distances between the sequences generated by the D-Markov Machines and the original\n",
    "#sequence.\n",
    "pdist_dmark = []\n",
    "for p_dm in p_dmark:\n",
    "    pdist_dm = sa.calc_pdist(p_dm, p, d_max)\n",
    "    pdist_dmark.append(pdist_dm)\n",
    "    \n",
    "pdist_dmark.to_csv(name + '/results/prob_distances/dmarkov_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If the Probability Distances of the D-Markov Machines have been previously computed, load them with this cell.\n",
    "if not pdist_dmark:\n",
    "    pdist_dmark = pd.read_csv(name + '/results/prob_distances/dmarkov_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Now that we have obtained the D-Markov Machines, the next step of DCGraM is to cluster the states of these machines. For a given D-Markov Machine *G*$_D$, its states $q$ are considered points in a $\\Sigma$-dimensional space, in which each dimension is labeled with a symbol $\\sigma$ from the alphabet and the position of the state $q$ in this dimension is its probability of transitioning with this symbol. These point-states are then clustered together in $K$ clusters using a variation of the K-Means clustering algorithm that instead of using an Euclidean distance between points, uses the Kullback-Leibler Divergence between the point-state and the cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustered = []\n",
    "K = 4\n",
    "for machine in dmark_machines:\n",
    "    clustered.append(clustering.kmeans_kld(machine, K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Minimization\n",
    "Once that the states of the D-Markov Machines are clustered, these clusterings are then used as initial partitions of the D-Markov Machines' states. To these machines and initial partitions, a graph minimization algorithm (in the current version, only Moore) is applied in order to obtain a final reduced PFSA, the DCGraM PFSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcgram_machines = []\n",
    "for ini_part in clustered:\n",
    "    dcgram_machines.append(graphmin.moore(clustered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGraM Analysis\n",
    "Now that the DCGraM machines have been generated, the same analysis done for the D-Markov Machines is used for them. Sequences are generated for each of the DCGraM machines and afterwards all of the analysis is applied to them so the comparison can be made between regular D-Markov and DCGraM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcgram_seqs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate sequences:\n",
    "count = 0\n",
    "for machine in dcgram_machines:\n",
    "    seq = machine.generate_sequence(N)\n",
    "    with open(name + '/sequences/dcgram_D' + str(drange[count]) + '_' + tag + '.yaml', 'w') as f:\n",
    "        yaml.dump(seq, f)\n",
    "    dcgram_seqs.append(seq)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If the sequences have been previously generated, load them here:\n",
    "if not dcgram_seqs:\n",
    "    for D in list(map(str,drange)):\n",
    "        with open(name + '/sequences/dcgram_D' + D + '_' + tag + '.yaml', 'w') as f:\n",
    "            dcgram_seqs.append(yaml.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute subsequence probabilities of occurrence of the DCGraM sequences\n",
    "count = 0\n",
    "p_dcgram = []\n",
    "for seq in dcgram_seqs:\n",
    "    p_dc, alphabet = sa.calc_probs(seq, d_max)\n",
    "    p_dc.to_csv(name + '/results/probabilities/dcgram_D'+ str(drange[count])  + '_' + tag + '.csv')\n",
    "    p_dcgram.append(p_dc)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If p_dcgram has been previously computed, use this cell to load the values\n",
    "if not p_dcgram:\n",
    "    for D in list(map(str,drange)):\n",
    "        p_dc = pd.read_csv(name + '/results/probabilities/dcgram_D' + D + '_' + tag + '.csv')\n",
    "        p_dcgram.append(p_dm)\n",
    "    with open(name + '/alphabet.yaml', 'r') as f:\n",
    "        alphabet = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute conditional probabilities of subsequences occurring after given each symbol of the alphabet\n",
    "#One of the two previous cells needs to be executed first.\n",
    "p_cond_dcgram = []\n",
    "count = 0\n",
    "if p_dcgram:\n",
    "    for p_dc in p_dcgram:\n",
    "        p_cond_dc = sa.calc_cond_probs(p_dc, alphabet, d_max) \n",
    "        p_cond_dc.to_csv(name + '/results/probabilities/conditional/dcgram_D' + str(drange[count]) + '_' + tag + '.csv')\n",
    "        p_cond_dcgram.append(p_cond_dc)\n",
    "        count += 1\n",
    "else:\n",
    "    print(\"Run a cell that either computes or opens the probabilities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If p_cond_dcgram has been previously computed, use this cell to load the values\n",
    "if not p_cond_dcgram:\n",
    "    for D in list(map(str,drange)):\n",
    "        p_cond_dcgram.append(pd.read_csv(name + '/results/probabilities/conditional/dcgram_D' + D + '_' + tag + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute conditional entropy\n",
    "count = 0\n",
    "h_dcgram = []\n",
    "if p_dcgram and p_cond_dcgram:\n",
    "    for p_dc in p_dcgram:\n",
    "        h_dc = sa.calc_cond_entropy(p_dc, p_cond_dcgram[count], d_max)\n",
    "        h_dc.to_csv(name + '/results/cond_entropies/dcgram_D' + str(drange[count]) + '_' + tag + '.csv')\n",
    "        h_dcgram.append(h_dc)\n",
    "        count += 1\n",
    "else:\n",
    "    print(\"Run the conditional probabilities cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If h_dcgram has been previously computed, use this cell to load the values\n",
    "if not h_dcgram:\n",
    "    for D in list(map(str,drange)):\n",
    "        h_dcgram.append(pd.read_csv(name + '/results/cond_entropies/dcgram_D' + D + '_' + tag + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute autocorrelation\n",
    "aut_dcgram = []\n",
    "count = 0\n",
    "for dcseq in dcgram_seqs:\n",
    "    aut_dc = sa.calc_autocorr(dcseq, a)\n",
    "    aut_dc.to_csv(name + '/results/autocorrelations/dcgram_D' + str(drange[count]) + '_'  + tag + '.csv')\n",
    "    aut_dcgram.append(aut_dc)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If aut has been previously computed, use this cell to load the values\n",
    "if not aut_dcgram:\n",
    "    for D in list(map(str,drange)):\n",
    "        aut_dmark.append(pd.read_csv(name + '/results/autocorrelations/dcgram_D' + D + '_' + tag + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute the Kullback-Leibler Divergence between the sequences generated by the DCGraM Machines and the original\n",
    "#sequence.\n",
    "kld_dcgram = []\n",
    "for dcseq in dcgram_seqs:\n",
    "    kld_dc = sa.calc_kld(dcseq, X, d_max)\n",
    "    kld_dcgram.append(kld_dc)\n",
    "    \n",
    "kld_dcgram.to_csv(name + '/results/kldivergences/dcgram_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If the DCGraM Kullback-Leibler divergence has been previously computed, use this cell to load the values\n",
    "if not kld_dcgram:\n",
    "    kld_dcgram = pd.read_csv(name + '/results/kldivergences/dcgram_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute the Probability Distances between the sequences generated by the DCGraM Machines and the original\n",
    "#sequence.\n",
    "pdist_dcgram = []\n",
    "for p_dc in p_dcgram:\n",
    "    pdist_dc = sa.calc_pdist(p_dc, p, d_max)\n",
    "    pdist_dcgram.append(pdist_dc)\n",
    "    \n",
    "pdist_dcgram.to_csv(name + '/results/prob_distances/dcgram_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If the Probability Distances of the DCGraM Machines have been previously computed, load them with this cell.\n",
    "if not pdist_dcgram:\n",
    "    pdist_dcgram = pd.read_csv(name + '/results/prob_distances/dcgram_' + tag + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "Once all analysis have been made, the plots of each of those parameters is created to visualize the performance. The plots have the x-axis representing the number of states of each PFSA and the y-axis represents the parameters being observed. There are always two curves: one for the DCGraM machines and one for the D-Markov Machines. Each point in these curves represents a machine of that type for a certain value of $D$. The further right a point is in the curve, the higher its $D$-value. On the curve for conditional entropy there is also a black representing the original sequence's conditional entropy for the $L$ being used as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Labels to be used in the plots' legends\n",
    "labels = ['D-Markov Machines, D from ' + str(drange[0]) + ' to ' + str(d_max),\n",
    "          'DCGraM Machines, D from ' + str(drange[0]) + ' to ' + str(d_max),\n",
    "          'Original Sequence Baseline']\n",
    "\n",
    "#Obtaining number of states of the machines to be used in the x-axis:\n",
    "states_dmarkov = []\n",
    "for dm in dmark_machines:\n",
    "    states_dmarkov.append(dm.shape[0])\n",
    "    \n",
    "states_dcgram = []\n",
    "for dc in dcgram_machines:\n",
    "    states_dcgram.append(dc.shape[0])\n",
    "    \n",
    "states = [states_dmarkov, states_dcgram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conditional Entropy plots\n",
    "\n",
    "H = 10\n",
    "\n",
    "h_dmark_curve = []\n",
    "for h_dm in h_dmarkov:\n",
    "    h_dmark_curve.append(h_dm[H])\n",
    "plt.semilogx(states[0], h_dmark_curve, marker='o', label=labels[0])\n",
    "    \n",
    "h_dcgram_curve = []\n",
    "for h_dc in h_dcgram:\n",
    "    h_dcgram_curve.append(h_dc[H])\n",
    "plt.semilogx(states[1], h_dcgram_curve, marker='x', label=labels[1])\n",
    "    \n",
    "\n",
    "#Opening original sequence baseline:\n",
    "h_base = h[H]\n",
    "plt.axhline(y=h_base, color='k', linewidth = 3, label=labels[2])\n",
    "\n",
    "plt.xlabel('Number of States', fontsize=16)\n",
    "plt.yalbel('$h_' + str(H) + '$', fontsize=16)\n",
    "plt.legend(loc='upper right', shadow=False, fontsize='large')\n",
    "plt.title('Conditional Entropy',fontsize=18,weight='bold')\n",
    "plt.savefig(name + '/plots/conditional_entropy_' + tag + '.eps' , bbox_inches='tight', format='eps',dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Kullback-Leibler plots\n",
    "\n",
    "plt.semilogx(states[0], kld_dmark, marker='o', label=labels[0])\n",
    "plt.semilogx(states[1], kld_dcgram, marker='x', label=labels[1])\n",
    "\n",
    "plt.xlabel('Number of States', fontsize=16)\n",
    "plt.yalbel('$k_' + str(H) + '$', fontsize=16)\n",
    "plt.legend(loc='upper right', shadow=False, fontsize='large')\n",
    "plt.title('Kullback-Leibler Divergence',fontsize=18,weight='bold')\n",
    "plt.savefig(name + '/plots/kldivergence_' + tag + '.eps' , bbox_inches='tight', format='eps',dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Probability Distance plots\n",
    "\n",
    "plt.semilogx(states[0], pdist_dmark, marker='o', label=labels[0])\n",
    "plt.semilogx(states[1], pdist_dcgram, marker='x', label=labels[1])\n",
    "\n",
    "plt.xlabel('Number of States', fontsize=16)\n",
    "plt.yalbel('$P_' + str(H) + '$', fontsize=16)\n",
    "plt.legend(loc='upper right', shadow=False, fontsize='large')\n",
    "plt.title('Probability Distance',fontsize=18,weight='bold')\n",
    "plt.savefig(name + '/plots/prob_distance_' + tag + '.eps' , bbox_inches='tight', format='eps',dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Think how to have good plots for autocorrelation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
